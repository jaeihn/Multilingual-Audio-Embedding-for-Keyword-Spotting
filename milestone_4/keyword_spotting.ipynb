{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fcf54138-d27a-4098-854e-ae136ee2ea89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperForAudioClassification, WhisperConfig\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import evaluate\n",
    "import librosa\n",
    "import wandb\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "from datasets import Dataset\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ffe1398-13ed-4c22-9cb7-389fcee28ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KWS_dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, input_data, output_data):\n",
    "        self.input_data = input_data\n",
    "        self.output_data = output_data\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.input_data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        keyword = self.output_data[index]\n",
    "        audio_features = self.input_data[index]\n",
    "        # return audio_features, keyword\n",
    "        return {'audio': audio_features,\n",
    "                'keyword': keyword\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "428b3c93-75d3-4915-871b-efadcefe62e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e34f271a-c86c-445d-94e3-15c0ffb233dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.load('../data/en_splits_30.trainloader')\n",
    "dev_dataloader = torch.load('../data/en_splits_30.devloader')\n",
    "test_dataloader = torch.load('../data/en_splits_30.testloader')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b032ef1-30d2-4764-8ddc-d3b3b65797c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a94003ae-e0c5-42f9-a647-630461bc2e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "294fcfef-2a56-4637-92c4-fe589027f6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "whisper_model = WhisperForAudioClassification(WhisperConfig(\n",
    "    num_mel_bins=80,\n",
    "    vocab_size=30,\n",
    "    num_labels=31,\n",
    "    max_source_positions=50,\n",
    "    classifier_proj_size=512,\n",
    "    encoder_layer=8,\n",
    "    decoder_layer=8,\n",
    "    dropout=0.2\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d835a549-522c-483b-b9f3-5ab07fe61dfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a3c12acd8ba4c8fa6c816417b9fecca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/2.50k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7a33191062f40e2afe92a5fe701cccb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/27.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "WhisperForAudioClassification(\n",
       "  (encoder): WhisperEncoder(\n",
       "    (conv1): Conv1d(80, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (conv2): Conv1d(256, 256, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "    (embed_positions): Embedding(50, 256)\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x WhisperEncoderLayer(\n",
       "        (self_attn): WhisperAttention(\n",
       "          (k_proj): Linear(in_features=256, out_features=256, bias=False)\n",
       "          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (activation_fn): GELUActivation()\n",
       "        (fc1): Linear(in_features=256, out_features=1536, bias=True)\n",
       "        (fc2): Linear(in_features=1536, out_features=256, bias=True)\n",
       "        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (projector): Linear(in_features=256, out_features=512, bias=True)\n",
       "  (classifier): Linear(in_features=512, out_features=31, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whisper_model.from_pretrained(\"jaeihn/kws_embedding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e3fb3d6d-363e-4635-83d9-088422217a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KWS_classifier(nn.Module):\n",
    "    def __init__(self, input_size=31, output_size=3):\n",
    "        super(KWS_classifier, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, output_size)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Freeze embedding weights\n",
    "        x = self.linear(x)\n",
    "        x = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "35fc3da6-3e0f-4cf9-9991-8fe543a238ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "got school way name work city however little right found may four much known years called alchemist make world come\n"
     ]
    }
   ],
   "source": [
    "with open(data_path + 'keywords_en_50.txt') as f:\n",
    "    keywords = [word.strip() for word in f.readlines()][30:]\n",
    "print(\" \".join(keywords))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b605ff4-26c0-49b7-a867-5310ac579c2b",
   "metadata": {},
   "source": [
    "```bash\n",
    "for word in got school way name work city however little right found may four much known years called alchemist make world come; \n",
    "do \n",
    "    python kws_preparation.py en/en_splits.csv keywords_en_30.txt $word; done\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "082000d9-0641-41bd-8742-567139f42301",
   "metadata": {},
   "outputs": [],
   "source": [
    "def keyword_spotting(word):\n",
    "    kws_train_dataloader = torch.load(data_path+word+'_128_kws.trainloader')\n",
    "    kws_dev_dataloader = torch.load(data_path+word+'_128_kws.devloader')\n",
    "\n",
    "    kws_model = KWS_classifier(input_size=31)\n",
    "    whisper_model.to(device)\n",
    "    optim = torch.optim.Adam(kws_model.parameters(),lr=0.01)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    \n",
    "    epochs = 100\n",
    "\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        kws_model.train()\n",
    "        whisper_model.eval()\n",
    "        for batch in kws_train_dataloader:\n",
    "            optim.zero_grad()\n",
    "            audio = batch['audio'].to(device)\n",
    "            labels = batch['keyword'].to(device)\n",
    "            outputs = whisper_model(audio)\n",
    "            outputs = kws_model(outputs.logits)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "        kws_model.eval()\n",
    "\n",
    "        for batch in kws_dev_dataloader:\n",
    "            audio = batch['audio'].to(device)\n",
    "            labels = batch['keyword'].to(device)\n",
    "            outputs = whisper_model(audio)\n",
    "            outputs = kws_model(outputs.logits)\n",
    "\n",
    "            metric.add_batch(predictions=outputs.argmax(-1), references=labels)\n",
    "    return metric.compute()['accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d896a6c-c486-4ee7-8273-c09183582d9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 100/100 [03:20<00:00,  2.01s/it]\n",
      "100%|█████████████████████████████████████████| 100/100 [03:19<00:00,  2.00s/it]\n",
      "100%|█████████████████████████████████████████| 100/100 [03:20<00:00,  2.00s/it]\n",
      "100%|█████████████████████████████████████████| 100/100 [04:30<00:00,  2.70s/it]\n",
      "100%|█████████████████████████████████████████| 100/100 [05:19<00:00,  3.20s/it]\n",
      "100%|█████████████████████████████████████████| 100/100 [05:18<00:00,  3.19s/it]\n",
      "100%|█████████████████████████████████████████| 100/100 [05:15<00:00,  3.16s/it]\n",
      "100%|█████████████████████████████████████████| 100/100 [05:16<00:00,  3.17s/it]\n",
      "100%|█████████████████████████████████████████| 100/100 [05:16<00:00,  3.17s/it]\n",
      "100%|█████████████████████████████████████████| 100/100 [05:18<00:00,  3.18s/it]\n",
      "100%|█████████████████████████████████████████| 100/100 [05:18<00:00,  3.19s/it]\n",
      "100%|█████████████████████████████████████████| 100/100 [05:20<00:00,  3.20s/it]\n",
      "100%|█████████████████████████████████████████| 100/100 [05:18<00:00,  3.19s/it]\n",
      "100%|█████████████████████████████████████████| 100/100 [05:18<00:00,  3.19s/it]\n",
      "100%|█████████████████████████████████████████| 100/100 [05:17<00:00,  3.18s/it]\n",
      " 84%|███████████████████████████████████▎      | 84/100 [04:26<00:51,  3.23s/it]"
     ]
    }
   ],
   "source": [
    "mono_en_accuracy = {}\n",
    "for word in keywords:\n",
    "    mono_en_accuracy[word] = keyword_spotting(word)\n",
    "print(\"COMPLETE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d0641eac-4d61-4c19-858e-865072375459",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|███▍                              | 5/50 [00:20<03:07,  4.16s/it]wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      " 16%|█████▍                            | 8/50 [00:33<02:55,  4.17s/it]wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      " 24%|███████▉                         | 12/50 [00:49<02:38,  4.17s/it]wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      " 30%|█████████▉                       | 15/50 [01:02<02:26,  4.20s/it]wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      " 38%|████████████▌                    | 19/50 [01:19<02:11,  4.25s/it]wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      " 44%|██████████████▌                  | 22/50 [01:32<01:57,  4.20s/it]wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      " 52%|█████████████████▏               | 26/50 [01:48<01:41,  4.21s/it]wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      " 58%|███████████████████▏             | 29/50 [02:01<01:28,  4.20s/it]wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      " 66%|█████████████████████▊           | 33/50 [02:18<01:11,  4.19s/it]wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      " 72%|███████████████████████▊         | 36/50 [02:30<00:58,  4.19s/it]wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      " 80%|██████████████████████████▍      | 40/50 [02:47<00:42,  4.22s/it]wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      " 96%|███████████████████████████████▋ | 48/50 [03:21<00:08,  4.22s/it]wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "100%|█████████████████████████████████| 50/50 [03:29<00:00,  4.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7992957746478874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "kws_train_dataloader = torch.load('../data/people_128_kws.trainloader')\n",
    "kws_dev_dataloader = torch.load('../data/people_128_kws.trainloader-1')\n",
    "\n",
    "\n",
    "kws_model = KWS_classifier(input_size=31)\n",
    "whisper_model.to(device)\n",
    "optim = torch.optim.Adam(kws_model.parameters(),lr=0.01)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# wandb.init(\n",
    "#     # set the wandb project where this run will be logged\n",
    "#     project=\"kws\",\n",
    "#     config= {\n",
    "#     \"architecture\": \"softmax\",\n",
    "#     \"dataset\": \"people\",\n",
    "#     \"epochs\": \"10\", \n",
    "#     }\n",
    "    \n",
    "# )\n",
    "\n",
    "epochs = 50\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    kws_model.train()\n",
    "    whisper_model.eval()\n",
    "    for batch in kws_train_dataloader:\n",
    "        optim.zero_grad()\n",
    "        audio = batch['audio'].to(device)\n",
    "        labels = batch['keyword'].to(device)\n",
    "        outputs = whisper_model(audio)\n",
    "        outputs = kws_model(outputs.logits)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "    kws_model.eval()\n",
    "        \n",
    "    for batch in kws_dev_dataloader:\n",
    "        audio = batch['audio'].to(device)\n",
    "        labels = batch['keyword'].to(device)\n",
    "        outputs = whisper_model(audio)\n",
    "        outputs = kws_model(outputs.logits)\n",
    "        \n",
    "        metric.add_batch(predictions=outputs.argmax(-1), references=labels)\n",
    "\n",
    "print(metric.compute()['accuracy'])\n",
    "    # wandb.log({\"acc\": metric.compute()['accuracy'], \"loss\": loss})\n",
    "    \n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b03b8c9-6263-4dac-a7ec-63857366c65c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
